{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Obtaining dependency information for unidecode from https://files.pythonhosted.org/packages/e4/63/7685ef40c65aba621ccd2524a24181bf11f0535ab1fdba47e40738eacff6/Unidecode-1.3.7-py3-none-any.whl.metadata\n",
      "  Downloading Unidecode-1.3.7-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/235.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/235.5 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/235.5 kB 393.8 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 81.9/235.5 kB 657.6 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 143.4/235.5 kB 944.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 225.3/235.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 235.5/235.5 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\repos\\vits\\notebooks\\huggingface_hub.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/repos/vits/notebooks/huggingface_hub.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/repos/vits/notebooks/huggingface_hub.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/repos/vits/notebooks/huggingface_hub.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munidecode\u001b[39;00m \u001b[39mimport\u001b[39;00m unidecode\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/repos/vits/notebooks/huggingface_hub.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m HARDCODED_MODEL_CONFIG \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/repos/vits/notebooks/huggingface_hub.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m:{\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/repos/vits/notebooks/huggingface_hub.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m#\"vocab_file\": \"lug/vocab.txt\",\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/vits/notebooks/huggingface_hub.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/vits/notebooks/huggingface_hub.ipynb#W0sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m }\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from unidecode import unidecode\n",
    "\n",
    "HARDCODED_MODEL_CONFIG = {\n",
    "    \"model\":{\n",
    "        #\"vocab_file\": \"lug/vocab.txt\",\n",
    "        #\"g_checkpoint_path\": \"/path/to/checkpoint\",#\n",
    "        #\"d_checkpoint_path\": \"/path/to/checkpoint\",\n",
    "        \"inter_channels\": 192,\n",
    "        \"hidden_channels\": 192,\n",
    "        \"filter_channels\": 768,\n",
    "        \"n_heads\": 2,\n",
    "        \"n_layers\": 6,\n",
    "        \"kernel_size\": 3,\n",
    "        \"p_dropout\": 0.1,\n",
    "        \"resblock\": \"1\",\n",
    "        \"resblock_kernel_sizes\": [3,7,11],\n",
    "        \"resblock_dilation_sizes\": [[1,3,5], [1,3,5], [1,3,5]],\n",
    "        \"upsample_rates\": [8,8,2,2],\n",
    "        \"upsample_initial_channel\": 512,\n",
    "        \"upsample_kernel_sizes\": [16,16,4,4],\n",
    "        \"n_layers_q\": 3,\n",
    "        \"use_spectral_norm\": False,\n",
    "        \"gin_channels\": 256\n",
    "    },\n",
    "    \"data\":{\n",
    "        #\"text_cleaners\":[\"custom_cleaners\"],\n",
    "        \"custom_cleaner_regex\": \":!\\?>\\.;,\", #LUG + ENG\n",
    "        \"max_wav_value\": 32768.0,\n",
    "        \"sampling_rate\": 16000,\n",
    "        \"filter_length\": 1024,\n",
    "        \"hop_length\": 256,\n",
    "        \"win_length\": 1024,\n",
    "        \"n_mel_channels\": 80,\n",
    "        \"mel_fmin\": 0.0,\n",
    "        \"mel_fmax\": None,\n",
    "        \"add_blank\": True,\n",
    "        \"n_speakers\": 109,\n",
    "        \"cleaned_text\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_whitespace_re = re.compile(r'\\s+')\n",
    "\n",
    "def collapse_whitespace(text):\n",
    "  return re.sub(_whitespace_re, ' ', text)\n",
    "\n",
    "def custom_add(text, regex):\n",
    "  regex = r\"[\" + regex + r\"]\"\n",
    "  text = re.sub(regex, ' ', text)\n",
    "  return text\n",
    "\n",
    "#Luganda and English\n",
    "def custom_cleaners(text):\n",
    "  text = text.lower()\n",
    "  text = custom_add(text, regex=HARDCODED_MODEL_CONFIG[\"data\"][\"custom_cleaner_regex\"] )\n",
    "  text = unidecode(text)\n",
    "  text = collapse_whitespace(text)\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMapper(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.symbols = [x.replace(\"\\n\", \"\") for x in open(vocab_file, encoding=\"utf-8\").readlines()]\n",
    "        self.SPACE_ID = self.symbols.index(\" \")\n",
    "        self._symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
    "        self._id_to_symbol = {i: s for i, s in enumerate(self.symbols)}\n",
    "\n",
    "\n",
    "    def text_to_sequence(self, text, cleaner):\n",
    "        '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "        Args:\n",
    "        text: string to convert to a sequence\n",
    "        cleaner_names: names of the cleaner functions to run the text through\n",
    "        Returns:\n",
    "        List of integers corresponding to the symbols in the text\n",
    "        '''\n",
    "        #sequence = []\n",
    "        #clean_text = text.strip()\n",
    "        sequence = []\n",
    "        clean_text = cleaner(text)\n",
    "        for symbol in clean_text:\n",
    "            symbol_id = self._symbol_to_id[symbol]\n",
    "            sequence += [symbol_id]\n",
    "        return sequence\n",
    "\n",
    "    @staticmethod\n",
    "    def intersperse(lst, item):\n",
    "        result = [item] * (len(lst) * 2 + 1)\n",
    "        result[1::2] = lst\n",
    "        return result\n",
    "\n",
    "\n",
    "    def get_text(self, text):\n",
    "        text_norm = self.text_to_sequence(text, custom_cleaners)\n",
    "        if HARDCODED_MODEL_CONFIG[\"data\"][\"add_blank\"]:\n",
    "            text_norm = self.intersperse(text_norm, 0)\n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def filter_oov(self, text):\n",
    "        val_chars = self._symbol_to_id\n",
    "        txt_filt = \"\".join(list(filter(lambda x: x in val_chars, text)))\n",
    "        #print(f\"text after filtering OOV: {txt_filt}\")\n",
    "        return txt_filt\n",
    "\n",
    "def preprocess_text(txt, text_mapper, lang=None):\n",
    "    txt = preprocess_char(txt, lang=lang)\n",
    "    # is_uroman = hps.data.training_files.split('.')[-1] == 'uroman'\n",
    "    # if is_uroman:\n",
    "    #     with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    #         if uroman_dir is None:\n",
    "    #             cmd = f\"git clone git@github.com:isi-nlp/uroman.git {tmp_dir}\"\n",
    "    #             print(cmd)\n",
    "    #             subprocess.check_output(cmd, shell=True)\n",
    "    #             uroman_dir = tmp_dir\n",
    "    #         uroman_pl = os.path.join(uroman_dir, \"bin\", \"uroman.pl\")\n",
    "    #         print(f\"uromanize\")\n",
    "    #         txt = text_mapper.uromanize(txt, uroman_pl)\n",
    "    #         print(f\"uroman text: {txt}\")\n",
    "    txt = txt.lower()\n",
    "    txt = text_mapper.filter_oov(txt)\n",
    "    return txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer=None):\n",
    "  assert os.path.isfile(checkpoint_path)\n",
    "  checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "  try:\n",
    "    iteration = checkpoint_dict['iteration']\n",
    "  except:\n",
    "    iteration = None\n",
    "  try:\n",
    "    learning_rate = checkpoint_dict['learning_rate']\n",
    "  except:\n",
    "    learning_rate = None\n",
    "  if optimizer is not None:\n",
    "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "  try:\n",
    "    saved_state_dict = checkpoint_dict['model']\n",
    "  except: \n",
    "    saved_state_dict = checkpoint_dict\n",
    "\n",
    "  if hasattr(model, 'module'):\n",
    "    state_dict = model.module.state_dict()\n",
    "  else:\n",
    "    state_dict = model.state_dict()\n",
    "  new_state_dict= {}\n",
    "  for k, v in state_dict.items():\n",
    "    try:\n",
    "      new_state_dict[k] = saved_state_dict[k]\n",
    "    except:\n",
    "      logger.info(\"%s is not in the checkpoint\" % k)\n",
    "      new_state_dict[k] = v\n",
    "  if hasattr(model, 'module'):\n",
    "    model.module.load_state_dict(new_state_dict)\n",
    "  else:\n",
    "    model.load_state_dict(new_state_dict)\n",
    "  logger.info(\"Loaded checkpoint '{}' (iteration {})\" .format(\n",
    "    checkpoint_path, iteration))\n",
    "  return model, optimizer, learning_rate, iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VITSInfereceAdapterModel:\n",
    "\n",
    "    def __init__(self, model_path, config_path, vocab_path, repo_name):\n",
    "        self.repo_name = repo_name\n",
    "        self.hps = self._download_and_load_config(config_path)\n",
    "        self.text_mapper = self._download_and_load_vocab(vocab_path)\n",
    "        self.model  = SynthesizerTrn(\n",
    "                len(self.text_mapper.symbols),\n",
    "                HARDCODED_MODEL_CONFIG[\"data\"[\"filter_length\"] // 2 + 1,\n",
    "                HARDCODED_MODEL_CONFIG[\"train\"][\"segment_size\"] // HARDCODED_MODEL_CONFIG[\"HARDCODED_MODEL_CONFIG\"][\"hop_length\"],\n",
    "                **HARDCODED_MODEL_CONFIG['model'])\n",
    "        self.net_g = self._download_and_load_model(model_path)\n",
    "\n",
    "    def _download_and_load_model(self, model_path, model, optimizer):\n",
    "        # Use hf_hub_download to download the model\n",
    "        model_file = hf_hub_download(repo_id=self.repo_name, filename=model_path)\n",
    "        # Load the model using your custom logic\n",
    "        load_checkpoint(model_file, model, optimizer)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    #def _download_and_load_config(self, config_path):\n",
    "    #    # Similar logic for downloading and loading the config\n",
    "    #    config_file = hf_hub_download(repo_id=self.repo_name, filename=model_path)\n",
    "\n",
    "    def _download_and_load_vocab(self, vocab_path):\n",
    "        # Similar logic for downloading and loading the vocab\n",
    "        vocab_file = hf_hub_download(repo_id=self.repo_name, filename=vocab_path)\n",
    "        text_mapper = TextMapper(vocab_file)\n",
    "        return text_mapper\n",
    "\n",
    "    def encode_text(self, txt):\n",
    "        txt = preprocess_text(txt, self.text_mapper, lang=LANG)\n",
    "        stn_tst = self.text_mapper.get_text(txt, hps)\n",
    "        with torch.no_grad():\n",
    "            x_tst = stn_tst.unsqueeze(0).to(device)\n",
    "            x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "            hyp = net_g.infer(\n",
    "                x_tst, x_tst_lengths,0, noise_scale=.667,\n",
    "                noise_scale_w=0.8, length_scale=1.0\n",
    "            )[0][0].detach().cpu()\n",
    "        return hyp\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, repo_name, G_net_path = \"G_eng_lug.pth\", vocab_path=\"vocab.txt\", config_path = None  ):\n",
    "        # Logic to instantiate the model using the repository name\n",
    "        #G_net_path = \"path_to_model_within_repo\"\n",
    "        return cls(G_net_path, config_path, vocab_path, repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
